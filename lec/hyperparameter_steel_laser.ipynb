{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "#Prediction of penetration depth in high power laser welding\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 5)):    #hidden layer의 개수를 2개부터 5개까지\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=16,\n",
    "                                            max_value=64,\n",
    "                                            step=16),    #layer의 node 개수를 16개부터 64개까지 16개의 간격으로\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),   #Adam에서 학습률은 3개 중에서\n",
    "        loss='mse',metrics=['mae'])\n",
    "    return model    \n",
    "\n",
    "''' SNN 모델 node 개수와 학습률에 대해서만 최적화하기\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    hp_units = hp.Int('units', min_value = 8, max_value = 64, step = 8)\n",
    "    model.add(layers.Dense(units = hp_units, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mse',metrics=['mae'])\n",
    "    return model              \n",
    "'''\n",
    "\n",
    "xy = np.loadtxt('C:/Temp/steel_laser.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "# 1: laser power, 2: Welding speed, 3: Beam diameter, 4: Penetration depth\n",
    "x_data = xy[:, 0:-1]\n",
    "#x_org = copy.deepcopy(x_data)\n",
    "\n",
    "# Normalizing data\n",
    "x_data -= x_data.mean(axis=0)\n",
    "x_data /= x_data.std(axis=0)\n",
    "\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "#원래는 데이터 나눠서 train데이터로 정규화해야함\n",
    "ymean = y_data.mean(axis=0)\n",
    "ystd = y_data.std(axis=0)\n",
    "\n",
    "y_data -= ymean\n",
    "y_data /= ystd \n",
    "\n",
    "# split data: Training data를 80%\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, \n",
    "\t\t\t\t       train_size=0.80, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 09s]\n",
      "val_mae: 0.15047350029150644\n",
      "\n",
      "Best val_mae So Far: 0.1288587599992752\n",
      "Total elapsed time: 00h 16m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Search space summary\n",
      "Default search space size: 7\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 5, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "Results summary\n",
      "Results in ./Data/Keras Tuner Test\n",
      "Showing 10 best trials\n",
      "Objective(name='val_mae', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 32\n",
      "units_1: 48\n",
      "learning_rate: 0.01\n",
      "units_2: 48\n",
      "units_3: 48\n",
      "units_4: 32\n",
      "Score: 0.1288587599992752\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 48\n",
      "units_1: 32\n",
      "learning_rate: 0.01\n",
      "units_2: 16\n",
      "units_3: 64\n",
      "units_4: 48\n",
      "Score: 0.12940683215856552\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 48\n",
      "units_1: 16\n",
      "learning_rate: 0.01\n",
      "units_2: 48\n",
      "units_3: 48\n",
      "units_4: 48\n",
      "Score: 0.12992012997468314\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 48\n",
      "units_1: 32\n",
      "learning_rate: 0.01\n",
      "units_2: 32\n",
      "units_3: 48\n",
      "units_4: 32\n",
      "Score: 0.13035175204277039\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 64\n",
      "units_1: 32\n",
      "learning_rate: 0.01\n",
      "units_2: 64\n",
      "units_3: 32\n",
      "Score: 0.13055449724197388\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 48\n",
      "units_1: 48\n",
      "learning_rate: 0.01\n",
      "units_2: 16\n",
      "units_3: 32\n",
      "units_4: 48\n",
      "Score: 0.13116377095381418\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 32\n",
      "units_1: 64\n",
      "learning_rate: 0.01\n",
      "units_2: 32\n",
      "units_3: 16\n",
      "units_4: 16\n",
      "Score: 0.13125314811865488\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 32\n",
      "units_1: 32\n",
      "learning_rate: 0.01\n",
      "units_2: 64\n",
      "units_3: 48\n",
      "units_4: 32\n",
      "Score: 0.13185336192448935\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 48\n",
      "units_1: 32\n",
      "learning_rate: 0.01\n",
      "units_2: 32\n",
      "units_3: 32\n",
      "units_4: 32\n",
      "Score: 0.13230125606060028\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 32\n",
      "units_1: 16\n",
      "learning_rate: 0.01\n",
      "units_2: 32\n",
      "units_3: 48\n",
      "units_4: 64\n",
      "Score: 0.13327946762243906\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=100,                                # 이거는 랜덤으로 100번까지 찾는 방법\n",
    "    executions_per_trial=3,\n",
    "    directory='./Data/',\n",
    "    project_name='Keras Tuner Test')\n",
    "\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=50,\n",
    "             validation_data=(x_val, y_val))                  \n",
    "                                    \n",
    "tuner.search_space_summary()\n",
    "tuner.results_summary()\n",
    "                  \n",
    "# check results\n",
    "models = tuner.get_best_models(num_models=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 237 Complete [00h 00m 19s]\n",
      "val_mae: 0.1390775442123413\n",
      "\n",
      "Best val_mae So Far: 0.11779389530420303\n",
      "Total elapsed time: 00h 12m 24s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "The hyperparameter search is complete. The optimal number of layer is 2\n",
      "The number of units in each hidden layer is\n",
      "32\n",
      "16\n",
      "The optimal learning rate for the optimizer is 0.01\n",
      "Epoch 1/200\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.5475 - mae: 0.5233 - val_loss: 0.2873 - val_mae: 0.4240\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.1614 - mae: 0.3076 - val_loss: 0.1519 - val_mae: 0.2734\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0830 - mae: 0.2128 - val_loss: 0.1328 - val_mae: 0.2450\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0695 - mae: 0.1900 - val_loss: 0.1182 - val_mae: 0.2374\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0632 - mae: 0.1774 - val_loss: 0.1330 - val_mae: 0.2720\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0767 - mae: 0.2028 - val_loss: 0.1054 - val_mae: 0.2209\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0611 - mae: 0.1768 - val_loss: 0.1054 - val_mae: 0.1982\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0404 - mae: 0.1429 - val_loss: 0.1094 - val_mae: 0.2479\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0591 - mae: 0.1764 - val_loss: 0.0968 - val_mae: 0.1951\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0538 - mae: 0.1710 - val_loss: 0.0908 - val_mae: 0.2049\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0709 - mae: 0.1878 - val_loss: 0.1468 - val_mae: 0.2322\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0707 - mae: 0.1804 - val_loss: 0.0889 - val_mae: 0.1900\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0538 - mae: 0.1631 - val_loss: 0.0970 - val_mae: 0.1921\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0547 - mae: 0.1542 - val_loss: 0.0910 - val_mae: 0.1828\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0347 - mae: 0.1305 - val_loss: 0.0866 - val_mae: 0.1966\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0476 - mae: 0.1515 - val_loss: 0.1019 - val_mae: 0.1883\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0506 - mae: 0.1585 - val_loss: 0.0822 - val_mae: 0.1768\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.0377 - mae: 0.1350 - val_loss: 0.0771 - val_mae: 0.1865\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0330 - mae: 0.1291 - val_loss: 0.0746 - val_mae: 0.1857\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0332 - mae: 0.1251 - val_loss: 0.0700 - val_mae: 0.1736\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0355 - mae: 0.1317 - val_loss: 0.0682 - val_mae: 0.1747\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0377 - mae: 0.1307 - val_loss: 0.0709 - val_mae: 0.1789\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0392 - mae: 0.1309 - val_loss: 0.0684 - val_mae: 0.1713\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0285 - mae: 0.1124 - val_loss: 0.0630 - val_mae: 0.1562\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0315 - mae: 0.1210 - val_loss: 0.0665 - val_mae: 0.1775\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0423 - mae: 0.1456 - val_loss: 0.0655 - val_mae: 0.1630\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0309 - mae: 0.1217 - val_loss: 0.0637 - val_mae: 0.1513\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.1119 - val_loss: 0.0606 - val_mae: 0.1576\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1302 - val_loss: 0.0637 - val_mae: 0.1702\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0376 - mae: 0.1388 - val_loss: 0.0811 - val_mae: 0.1718\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0352 - mae: 0.1295 - val_loss: 0.0575 - val_mae: 0.1484\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0343 - mae: 0.1277 - val_loss: 0.0561 - val_mae: 0.1462\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.1181 - val_loss: 0.0664 - val_mae: 0.1591\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1208 - val_loss: 0.0544 - val_mae: 0.1479\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1149 - val_loss: 0.0583 - val_mae: 0.1497\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0307 - mae: 0.1235 - val_loss: 0.0641 - val_mae: 0.1482\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1146 - val_loss: 0.0537 - val_mae: 0.1401\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0326 - mae: 0.1303 - val_loss: 0.0569 - val_mae: 0.1462\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0267 - mae: 0.1183 - val_loss: 0.0729 - val_mae: 0.1565\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0348 - mae: 0.1319 - val_loss: 0.0642 - val_mae: 0.1769\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0340 - mae: 0.1357 - val_loss: 0.0633 - val_mae: 0.1445\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0310 - mae: 0.1202 - val_loss: 0.0575 - val_mae: 0.1504\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0251 - mae: 0.1109 - val_loss: 0.0601 - val_mae: 0.1801\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0278 - mae: 0.1185 - val_loss: 0.0528 - val_mae: 0.1498\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0266 - mae: 0.1185 - val_loss: 0.0507 - val_mae: 0.1466\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.1176 - val_loss: 0.0505 - val_mae: 0.1479\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.1303 - val_loss: 0.0536 - val_mae: 0.1406\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1077 - val_loss: 0.0493 - val_mae: 0.1377\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0212 - mae: 0.0994 - val_loss: 0.0636 - val_mae: 0.1793\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0257 - mae: 0.1197 - val_loss: 0.0536 - val_mae: 0.1484\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0235 - mae: 0.1106 - val_loss: 0.0498 - val_mae: 0.1387\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0217 - mae: 0.1004 - val_loss: 0.0632 - val_mae: 0.1486\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.1184 - val_loss: 0.0530 - val_mae: 0.1439\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.1147 - val_loss: 0.0527 - val_mae: 0.1384\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.1069 - val_loss: 0.0524 - val_mae: 0.1364\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0194 - mae: 0.0990 - val_loss: 0.0462 - val_mae: 0.1427\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0235 - mae: 0.1069 - val_loss: 0.0506 - val_mae: 0.1360\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0300 - mae: 0.1163 - val_loss: 0.0594 - val_mae: 0.1391\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0216 - mae: 0.0992 - val_loss: 0.0512 - val_mae: 0.1596\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0290 - mae: 0.1165 - val_loss: 0.0435 - val_mae: 0.1502\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0276 - mae: 0.1178 - val_loss: 0.0584 - val_mae: 0.1733\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0292 - mae: 0.1140 - val_loss: 0.0741 - val_mae: 0.2018\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0352 - mae: 0.1265 - val_loss: 0.0487 - val_mae: 0.1382\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0269 - mae: 0.1145 - val_loss: 0.0485 - val_mae: 0.1493\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0227 - mae: 0.1067 - val_loss: 0.0427 - val_mae: 0.1391\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0225 - mae: 0.1044 - val_loss: 0.0565 - val_mae: 0.1473\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0293 - mae: 0.1168 - val_loss: 0.0470 - val_mae: 0.1550\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0230 - mae: 0.1085 - val_loss: 0.0480 - val_mae: 0.1408\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0216 - mae: 0.1023 - val_loss: 0.0518 - val_mae: 0.1510\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0203 - mae: 0.1007 - val_loss: 0.0418 - val_mae: 0.1321\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0189 - mae: 0.0955 - val_loss: 0.0420 - val_mae: 0.1351\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0187 - mae: 0.0982 - val_loss: 0.0413 - val_mae: 0.1329\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0192 - mae: 0.0929 - val_loss: 0.0432 - val_mae: 0.1489\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0193 - mae: 0.1007 - val_loss: 0.0539 - val_mae: 0.1423\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0240 - mae: 0.1018 - val_loss: 0.0457 - val_mae: 0.1414\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0223 - mae: 0.1052 - val_loss: 0.0456 - val_mae: 0.1416\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0202 - mae: 0.1044 - val_loss: 0.0427 - val_mae: 0.1383\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0211 - mae: 0.1044 - val_loss: 0.0388 - val_mae: 0.1340\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0207 - mae: 0.1056 - val_loss: 0.0650 - val_mae: 0.1579\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0275 - mae: 0.1143 - val_loss: 0.0444 - val_mae: 0.1334\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0215 - mae: 0.1040 - val_loss: 0.0409 - val_mae: 0.1275\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0211 - mae: 0.1056 - val_loss: 0.0435 - val_mae: 0.1372\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0217 - mae: 0.0973 - val_loss: 0.0392 - val_mae: 0.1344\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0208 - mae: 0.1063 - val_loss: 0.0440 - val_mae: 0.1444\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0179 - mae: 0.0949 - val_loss: 0.0509 - val_mae: 0.1612\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1100 - val_loss: 0.0434 - val_mae: 0.1510\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.1062 - val_loss: 0.0471 - val_mae: 0.1415\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0198 - mae: 0.0963 - val_loss: 0.0424 - val_mae: 0.1309\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.0994 - val_loss: 0.0412 - val_mae: 0.1321\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0145 - mae: 0.0822 - val_loss: 0.0481 - val_mae: 0.1300\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0224 - mae: 0.1052 - val_loss: 0.0586 - val_mae: 0.1576\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0211 - mae: 0.1048 - val_loss: 0.0397 - val_mae: 0.1317\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0154 - mae: 0.0894 - val_loss: 0.0427 - val_mae: 0.1433\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0248 - mae: 0.1067 - val_loss: 0.0410 - val_mae: 0.1446\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0171 - mae: 0.0951 - val_loss: 0.0443 - val_mae: 0.1443\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0925 - val_loss: 0.0450 - val_mae: 0.1340\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0951 - val_loss: 0.0449 - val_mae: 0.1455\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.1026 - val_loss: 0.0420 - val_mae: 0.1278\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0165 - mae: 0.0894 - val_loss: 0.0412 - val_mae: 0.1396\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.0931 - val_loss: 0.0426 - val_mae: 0.1294\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0826 - val_loss: 0.0410 - val_mae: 0.1277\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0826 - val_loss: 0.0451 - val_mae: 0.1404\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0191 - mae: 0.0998 - val_loss: 0.0398 - val_mae: 0.1344\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0848 - val_loss: 0.0450 - val_mae: 0.1475\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.1091 - val_loss: 0.0433 - val_mae: 0.1438\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0209 - mae: 0.1028 - val_loss: 0.0389 - val_mae: 0.1334\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0179 - mae: 0.0942 - val_loss: 0.0365 - val_mae: 0.1297\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0234 - mae: 0.1119 - val_loss: 0.0435 - val_mae: 0.1494\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0237 - mae: 0.1129 - val_loss: 0.0409 - val_mae: 0.1300\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0171 - mae: 0.0933 - val_loss: 0.0471 - val_mae: 0.1425\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0942 - val_loss: 0.0378 - val_mae: 0.1302\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0164 - mae: 0.0892 - val_loss: 0.0369 - val_mae: 0.1277\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0163 - mae: 0.0869 - val_loss: 0.0379 - val_mae: 0.1316\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0967 - val_loss: 0.0361 - val_mae: 0.1260\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0172 - mae: 0.0941 - val_loss: 0.0544 - val_mae: 0.1567\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0226 - mae: 0.0997 - val_loss: 0.0388 - val_mae: 0.1264\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0162 - mae: 0.0885 - val_loss: 0.0371 - val_mae: 0.1302\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0201 - mae: 0.0978 - val_loss: 0.0365 - val_mae: 0.1303\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.0946 - val_loss: 0.0499 - val_mae: 0.1398\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0961 - val_loss: 0.0369 - val_mae: 0.1255\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0875 - val_loss: 0.0370 - val_mae: 0.1254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0192 - mae: 0.0947 - val_loss: 0.0511 - val_mae: 0.1724\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0261 - mae: 0.1118 - val_loss: 0.0492 - val_mae: 0.1648\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0250 - mae: 0.1248 - val_loss: 0.0398 - val_mae: 0.1437\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0207 - mae: 0.1086 - val_loss: 0.0554 - val_mae: 0.1645\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0247 - mae: 0.1058 - val_loss: 0.0430 - val_mae: 0.1530\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0236 - mae: 0.1125 - val_loss: 0.0550 - val_mae: 0.1622\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0222 - mae: 0.1131 - val_loss: 0.0500 - val_mae: 0.1454\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.1019 - val_loss: 0.0399 - val_mae: 0.1335\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0144 - mae: 0.0829 - val_loss: 0.0405 - val_mae: 0.1301\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1072 - val_loss: 0.0427 - val_mae: 0.1456\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0185 - mae: 0.0972 - val_loss: 0.0367 - val_mae: 0.1255\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0173 - mae: 0.0935 - val_loss: 0.0373 - val_mae: 0.1320\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0218 - mae: 0.0936 - val_loss: 0.0357 - val_mae: 0.1261\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0149 - mae: 0.0855 - val_loss: 0.0403 - val_mae: 0.1385\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0926 - val_loss: 0.0466 - val_mae: 0.1394\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.0913 - val_loss: 0.0387 - val_mae: 0.1385\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0976 - val_loss: 0.0379 - val_mae: 0.1383\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0952 - val_loss: 0.0399 - val_mae: 0.1458\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0153 - mae: 0.0905 - val_loss: 0.0403 - val_mae: 0.1403\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.0989 - val_loss: 0.0413 - val_mae: 0.1482\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0197 - mae: 0.1001 - val_loss: 0.0467 - val_mae: 0.1404\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1068 - val_loss: 0.0421 - val_mae: 0.1417\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0174 - mae: 0.0982 - val_loss: 0.0392 - val_mae: 0.1272\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0148 - mae: 0.0868 - val_loss: 0.0387 - val_mae: 0.1288\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0191 - mae: 0.0970 - val_loss: 0.0536 - val_mae: 0.1483\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0251 - mae: 0.1131 - val_loss: 0.0387 - val_mae: 0.1381\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0192 - mae: 0.0996 - val_loss: 0.0365 - val_mae: 0.1313\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0174 - mae: 0.0942 - val_loss: 0.0408 - val_mae: 0.1443\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0163 - mae: 0.0922 - val_loss: 0.0358 - val_mae: 0.1291\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0945 - val_loss: 0.0375 - val_mae: 0.1279\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0176 - mae: 0.0917 - val_loss: 0.0386 - val_mae: 0.1373\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0154 - mae: 0.0857 - val_loss: 0.0372 - val_mae: 0.1338\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0171 - mae: 0.0884 - val_loss: 0.0364 - val_mae: 0.1314\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0138 - mae: 0.0823 - val_loss: 0.0442 - val_mae: 0.1374\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0179 - mae: 0.0899 - val_loss: 0.0374 - val_mae: 0.1310\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0183 - mae: 0.0880 - val_loss: 0.0406 - val_mae: 0.1368\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0153 - mae: 0.0801 - val_loss: 0.0442 - val_mae: 0.1570\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0187 - mae: 0.0999 - val_loss: 0.0456 - val_mae: 0.1506\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0200 - mae: 0.0999 - val_loss: 0.0431 - val_mae: 0.1497\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0163 - mae: 0.0954 - val_loss: 0.0397 - val_mae: 0.1465\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0185 - mae: 0.0939 - val_loss: 0.0406 - val_mae: 0.1372\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0193 - mae: 0.0964 - val_loss: 0.0415 - val_mae: 0.1487\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0262 - mae: 0.1111 - val_loss: 0.0400 - val_mae: 0.1401\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0206 - mae: 0.0973 - val_loss: 0.0389 - val_mae: 0.1330\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0913 - val_loss: 0.0572 - val_mae: 0.1526\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0194 - mae: 0.0985 - val_loss: 0.0387 - val_mae: 0.1325\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0841 - val_loss: 0.0398 - val_mae: 0.1273\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0123 - mae: 0.0793 - val_loss: 0.0363 - val_mae: 0.1305\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0912 - val_loss: 0.0380 - val_mae: 0.1381\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0207 - mae: 0.1027 - val_loss: 0.0362 - val_mae: 0.1298\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0176 - mae: 0.0949 - val_loss: 0.0386 - val_mae: 0.1363\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0163 - mae: 0.0861 - val_loss: 0.0373 - val_mae: 0.1262\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0131 - mae: 0.0763 - val_loss: 0.0362 - val_mae: 0.1299\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0110 - mae: 0.0740 - val_loss: 0.0365 - val_mae: 0.1270\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0139 - mae: 0.0777 - val_loss: 0.0374 - val_mae: 0.1308\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0134 - mae: 0.0792 - val_loss: 0.0375 - val_mae: 0.1333\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0827 - val_loss: 0.0412 - val_mae: 0.1501\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0256 - mae: 0.1139 - val_loss: 0.0466 - val_mae: 0.1650\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0198 - mae: 0.1016 - val_loss: 0.0433 - val_mae: 0.1436\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0203 - mae: 0.0983 - val_loss: 0.0551 - val_mae: 0.1745\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0215 - mae: 0.1036 - val_loss: 0.0388 - val_mae: 0.1476\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0150 - mae: 0.0902 - val_loss: 0.0372 - val_mae: 0.1368\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0917 - val_loss: 0.0442 - val_mae: 0.1376\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0930 - val_loss: 0.0387 - val_mae: 0.1385\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0152 - mae: 0.0842 - val_loss: 0.0499 - val_mae: 0.1575\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.0899 - val_loss: 0.0391 - val_mae: 0.1349\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0827 - val_loss: 0.0472 - val_mae: 0.1427\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0204 - mae: 0.0994 - val_loss: 0.0440 - val_mae: 0.1362\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0141 - mae: 0.0770 - val_loss: 0.0364 - val_mae: 0.1269\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0188 - mae: 0.0945 - val_loss: 0.0402 - val_mae: 0.1394\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0930 - val_loss: 0.0440 - val_mae: 0.1371\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0228 - mae: 0.1103 - val_loss: 0.0365 - val_mae: 0.1311\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0183 - mae: 0.0896 - val_loss: 0.0425 - val_mae: 0.1373\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0924 - val_loss: 0.0360 - val_mae: 0.1287\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0121 - mae: 0.0746 - val_loss: 0.0392 - val_mae: 0.1279\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0128 - mae: 0.0794 - val_loss: 0.0368 - val_mae: 0.1357\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0160 - mae: 0.0891 - val_loss: 0.0444 - val_mae: 0.1427\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.1015 - val_loss: 0.0357 - val_mae: 0.1351\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0201 - mae: 0.0974 - val_loss: 0.0384 - val_mae: 0.1400\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABLOklEQVR4nO3dd3hUVfrA8e+bSUJ6gISa0KW3gAFUFEFRwQIWXGBdFRvi2l1ddd1d2eLuT9fddbEh9o4VREFRUUAFhNB7DxBCCQFSIG1mzu+Pc5NMkgkkIZMAvp/nyTMz995z58ydyX3vqVeMMSillFLlBdV3BpRSSp2cNEAopZTySwOEUkopvzRAKKWU8ksDhFJKKb80QCillPJLA4RStUBE3hCRv1dx21QRGXqi+1Eq0DRAKKWU8ksDhFJKKb80QKhfDKdq5yERWSUiR0TkVRFpJiJfikiOiHwrIo18th8hImtF5LCIzBWRrj7r+ojIMifdB0BYufe6XERWOGkXiEivGub5NhHZIiIHRWSGiLR0louI/FdE9otIlvOZejjrLhWRdU7edovIgzU6YOoXTwOE+qW5BrgI6ARcAXwJ/AGIx/4/3AMgIp2A94H7gCbALOBzEQkVkVBgOvA20Bj4yNkvTtq+wGvA7UAc8BIwQ0QaVCejInIB8E/gV0ALYAcw1Vl9MTDI+RwNgdFAprPuVeB2Y0w00AP4rjrvq1QxDRDql+ZZY8w+Y8xu4AfgZ2PMcmNMATAN6ONsNxqYaYz5xhhTBDwNhAPnAGcBIcAzxpgiY8zHwBKf97gNeMkY87MxxmOMeRMocNJVx3XAa8aYZU7+HgXOFpG2QBEQDXQBxBiz3hizx0lXBHQTkRhjzCFjzLJqvq9SgAYI9cuzz+d5np/XUc7zltgrdgCMMV5gF5DgrNttys50ucPneRvgd0710mEROQy0ctJVR/k85GJLCQnGmO+A54DngX0iMkVEYpxNrwEuBXaIyDwRObua76sUoAFCqcqkY0/0gK3zx57kdwN7gARnWbHWPs93AU8YYxr6/EUYY94/wTxEYqusdgMYYyYZY84EumOrmh5yli8xxowEmmKrwj6s5vsqBWiAUKoyHwKXiciFIhIC/A5bTbQAWAi4gXtEJFhErgb6+6R9GZggIgOcxuRIEblMRKKrmYf3gJtEJMlpv/gHtkosVUT6OfsPAY4A+YDHaSO5TkRinaqxbMBzAsdB/YJpgFDKD2PMRuA3wLPAAWyD9hXGmEJjTCFwNTAOOIRtr/jUJ20Kth3iOWf9Fmfb6uZhDvAn4BNsqaUDMMZZHYMNRIew1VCZ2HYSgOuBVBHJBiY4n0OpahO9YZBSSil/tAShlFLKLw0QSiml/NIAoZRSyi8NEEoppfwKru8M1Kb4+HjTtm3b+s6GUkqdMpYuXXrAGNPE37rTKkC0bduWlJSU+s6GUkqdMkRkR2XrtIpJKaWUXxoglFJK+aUBQimllF+nVRuEUqpuFRUVkZaWRn5+fn1nRR1HWFgYiYmJhISEVDmNBgilVI2lpaURHR1N27ZtKTu5rTqZGGPIzMwkLS2Ndu3aVTmdVjEppWosPz+fuLg4DQ4nOREhLi6u2iU9DRBKqROiweHUUJPvSQMEMGnOZuZtyqjvbCil1ElFAwTw4tyt/LhZA4RSp5rMzEySkpJISkqiefPmJCQklLwuLCw8ZtqUlBTuueee477HOeecUyt5nTt3Lpdffnmt7KuuaCM14AoSPN76zoVSqrri4uJYsWIFABMnTiQqKooHH3ywZL3b7SY42P9pLjk5meTk5OO+x4IFC2olr6ciLUEAQQJevXGSUqeFcePG8cADDzBkyBAefvhhFi9ezDnnnEOfPn0455xz2LhxI1D2in7ixIncfPPNDB48mPbt2zNp0qSS/UVFRZVsP3jwYEaNGkWXLl247rrrKL7h2qxZs+jSpQvnnnsu99xzz3FLCgcPHuTKK6+kV69enHXWWaxatQqAefPmlZSA+vTpQ05ODnv27GHQoEEkJSXRo0cPfvjhh1o/ZpXREgTFJQgNEEqdiL98vpZ16dm1us9uLWN4/Iru1U63adMmvv32W1wuF9nZ2cyfP5/g4GC+/fZb/vCHP/DJJ59USLNhwwa+//57cnJy6Ny5M3fccUeFMQPLly9n7dq1tGzZkoEDB/LTTz+RnJzM7bffzvz582nXrh1jx449bv4ef/xx+vTpw/Tp0/nuu++44YYbWLFiBU8//TTPP/88AwcOJDc3l7CwMKZMmcIll1zCY489hsfj4ejRo9U+HjUV0BKEiAwTkY0iskVEHjnGdv1ExCMio6qbtja4ggSPliCUOm1ce+21uFwuALKysrj22mvp0aMH999/P2vXrvWb5rLLLqNBgwbEx8fTtGlT9u3bV2Gb/v37k5iYSFBQEElJSaSmprJhwwbat29fMr6gKgHixx9/5PrrrwfgggsuIDMzk6ysLAYOHMgDDzzApEmTOHz4MMHBwfTr14/XX3+diRMnsnr1aqKjo2t6WKotYCUIEXEBzwMXAWnAEhGZYYxZ52e7J4HZ1U1bW4JE8GoJQqkTUpMr/UCJjIwsef6nP/2JIUOGMG3aNFJTUxk8eLDfNA0aNCh57nK5cLvdVdrG1ODi0l8aEeGRRx7hsssuY9asWZx11ll8++23DBo0iPnz5zNz5kyuv/56HnroIW644YZqv2dNBLIE0R/YYozZZowpBKYCI/1sdzfwCbC/BmlrhVYxKXX6ysrKIiEhAYA33nij1vffpUsXtm3bRmpqKgAffPDBcdMMGjSId999F7BtG/Hx8cTExLB161Z69uzJww8/THJyMhs2bGDHjh00bdqU2267jVtuuYVly5bV+meoTCADRAKwy+d1mrOshIgkAFcBk6ub1mcf40UkRURSMjJq1lU1SLSKSanT1e9//3seffRRBg4ciMfjqfX9h4eH88ILLzBs2DDOPfdcmjVrRmxs7DHTTJw4kZSUFHr16sUjjzzCm2++CcAzzzxDjx496N27N+Hh4QwfPpy5c+eWNFp/8skn3HvvvbX+GSojNSkeVWnHItcClxhjbnVeXw/0N8bc7bPNR8C/jTGLROQN4AtjzMdVSetPcnKyqckNgwY99T19WzfkmTF9qp1WqV+y9evX07Vr1/rORr3Lzc0lKioKYwx33nknHTt25P7776/vbFXg7/sSkaXGGL/9fQPZiykNaOXzOhFIL7dNMjDVGQIeD1wqIu4qpq01tpE6UHtXSp3uXn75Zd58800KCwvp06cPt99+e31nqVYEMkAsATqKSDtgNzAG+LXvBsaYkmkFfUoQ00Uk+Hhpa1OQoI3USqkau//++0/KEsOJCliAMMa4ReQubO8kF/CaMWatiExw1pdvdzhu2kDlVRuplVKqooAOlDPGzAJmlVvmNzAYY8YdL22gaCO1UkpVpFNtYEsQWsWklFJlaYBASxBKKeWPBgggKEjQAoRSp57Bgwcze/bsMsueeeYZfvvb3x4zTXF3+EsvvZTDhw9X2GbixIk8/fTTx3zv6dOns25d6eQOf/7zn/n222+rkXv/TqZpwTVAAC7txaTUKWns2LFMnTq1zLKpU6dWaT4ksLOwNmzYsEbvXT5A/PWvf2Xo0KE12tfJSgME2otJqVPVqFGj+OKLLygoKAAgNTWV9PR0zj33XO644w6Sk5Pp3r07jz/+uN/0bdu25cCBAwA88cQTdO7cmaFDh5ZMCQ52jEO/fv3o3bs311xzDUePHmXBggXMmDGDhx56iKSkJLZu3cq4ceP4+OOPAZgzZw59+vShZ8+e3HzzzSX5a9u2LY8//jh9+/alZ8+ebNiw4Zifr76nBdfpvtE2CKVqxZePwN7VtbvP5j1h+P9VujouLo7+/fvz1VdfMXLkSKZOncro0aMREZ544gkaN26Mx+PhwgsvZNWqVfTq1cvvfpYuXcrUqVNZvnw5brebvn37cuaZZwJw9dVXc9tttwHwxz/+kVdffZW7776bESNGcPnllzNq1Kgy+8rPz2fcuHHMmTOHTp06ccMNN/Diiy9y3333ARAfH8+yZct44YUXePrpp3nllVcq/Xz1PS24liDQXkxKncp8q5l8q5c+/PBD+vbtS58+fVi7dm2Z6qDyfvjhB6666ioiIiKIiYlhxIgRJevWrFnDeeedR8+ePXn33XcrnS682MaNG2nXrh2dOnUC4MYbb2T+/Pkl66+++moAzjzzzJIJ/ipT39OCawkCvR+EUrXiGFf6gXTllVfywAMPsGzZMvLy8ujbty/bt2/n6aefZsmSJTRq1Ihx48aRn59/zP04U/5UMG7cOKZPn07v3r154403mDt37jH3c7z57YqnDK9sSvHj7asupwXXEgR6PwilTmVRUVEMHjyYm2++uaT0kJ2dTWRkJLGxsezbt48vv/zymPsYNGgQ06ZNIy8vj5ycHD7//POSdTk5ObRo0YKioqKSKboBoqOjycnJqbCvLl26kJqaypYtWwB4++23Of/882v02ep7WnAtQaAlCKVOdWPHjuXqq68uqWrq3bs3ffr0oXv37rRv356BAwceM33fvn0ZPXo0SUlJtGnThvPOO69k3d/+9jcGDBhAmzZt6NmzZ0lQGDNmDLfddhuTJk0qaZwGCAsL4/XXX+faa6/F7XbTr18/JkyYUKPPNXHiRG666SZ69epFREREmWnBv//+e1wuF926dWP48OFMnTqVf/3rX4SEhBAVFcVbb71Vo/f0FbDpvutDTaf7vvXNFHYfzuPLe887/sZKqRI63feppbrTfWsVE+AK0nEQSilVngYItIpJKaX80QCBNlIrdSJOp2rq01lNvicNEGgJQqmaCgsLIzMzU4PESc4YQ2ZmJmFhYdVKp72YAJfoVBtK1URiYiJpaWlkZGTUd1bUcYSFhZGYmFitNAENECIyDPgf9q5wrxhj/q/c+pHA3wAv4AbuM8b86KxLBXIAD+CurJW9NgTpSGqlaiQkJIR27dodf0N1SgpYgBARF/A8cBGQBiwRkRnGGN/x7nOAGcYYIyK9gA+BLj7rhxhjDgQqj8VcOheTUkpVEMg2iP7AFmPMNmNMITAVGOm7gTEm15RWXkYC9XKWDgoSPN76eGellDp5BTJAJAC7fF6nOcvKEJGrRGQDMBO42WeVAb4WkaUiMr6yNxGR8SKSIiIpNa0HdQWBV0sQSilVRiADhL+ZryqchY0x04wxXYArse0RxQYaY/oCw4E7RWSQvzcxxkwxxiQbY5KbNGlSo4xqI7VSSlUUyACRBrTyeZ0IpFe2sTFmPtBBROKd1+nO435gGrbKKiC0kVoppSoKZIBYAnQUkXYiEgqMAWb4biAiZ4gzx66I9AVCgUwRiRSRaGd5JHAxsCZQGdVGaqWUqihgvZiMMW4RuQuYje3m+poxZq2ITHDWTwauAW4QkSIgDxjt9GhqBkxzYkcw8J4x5qtA5VVvOaqUUhUFdByEMWYWMKvcssk+z58EnvSTbhvQO5B58xUUJNpIrZRS5ehUG2gjtVJK+aMBguIShE46ppRSvjRAYEsQAFqIUEqpUhogsAPlAK1mUkopHxogACkpQWiAUEqpYhogsN1cQUsQSinlSwMEvm0QGiCUUqqYBghsLyYAr87oqpRSJTRAAC5nWkGdbkMppUppgEDbIJRSyh8NEPhUMWkJQimlSmiAoLSRWksQSilVSgMEpSUIDRBKKVVKAwTazVUppfzRAIE2UiullD8BDRAiMkxENorIFhF5xM/6kSKySkRWiEiKiJxb1bS1SRuplVKqooAFCBFxAc8Dw4FuwFgR6VZuszlAb2NMEnAz8Eo10taa0kbqQL2DUkqdegJZgugPbDHGbDPGFAJTgZG+Gxhjck3pTRgiAVPVtLVJZ3NVSqmKAhkgEoBdPq/TnGVliMhVIrIBmIktRVQ5rZN+vFM9lZKRkVGjjAZpI7VSSlUQyAAhfpZVOAMbY6YZY7oAVwJ/q05aJ/0UY0yyMSa5SZMmNcqoNlIrpVRFgQwQaUArn9eJQHplGxtj5gMdRCS+umlPVMk4CC1BKKVUiUAGiCVARxFpJyKhwBhghu8GInKGOHfrEZG+QCiQWZW0talkHISWIJRSqkRwoHZsjHGLyF3AbMAFvGaMWSsiE5z1k4FrgBtEpAjIA0Y7jdZ+0wYqr1rFpJRSFQUsQAAYY2YBs8otm+zz/EngyaqmDZTwI7tpRLZWMSmllA8dSQ30nnExtwd/oTcMUkopHxogAOMKIRS3liCUUsqHBgjABIUSglsbqZVSyocGCMAEhRCCWxuplVLKhwYIgKAQQkSrmJRSypcGCErbILSKSSmlSmmAAHCFEIJHSxBKKeVDAwRgXKEEaxuEUkqVoQECICjUVjFpCUIppUpogIDSKiYdKKeUUiU0QAC4QgkRbaRWSilfGiDAKUFoFZNSSvnSAAGgU20opVQFGiAAXKEE49EqJqWU8qEBAhBXqE61oZRS5WiAACQ41Jlqo75zopRSJ4+ABggRGSYiG0Vki4g84mf9dSKyyvlbICK9fdalishqEVkhIimBzCc61YZSSlUQsDvKiYgLeB64CEgDlojIDGPMOp/NtgPnG2MOichwYAowwGf9EGPMgUDlsSSvwU4VkzZSK6VUiUCWIPoDW4wx24wxhcBUYKTvBsaYBcaYQ87LRUBiAPNTKW2DUEqpigIZIBKAXT6v05xllbkF+NLntQG+FpGlIjK+skQiMl5EUkQkJSMjo0YZleJxEBoglFKqRMCqmADxs8zvGVhEhmADxLk+iwcaY9JFpCnwjYhsMMbMr7BDY6Zgq6ZITk6u0Rk+KDgUl3jw6E2plVKqRCBLEGlAK5/XiUB6+Y1EpBfwCjDSGJNZvNwYk+487gemYausAkKCQ+0TjztQb6GUUqecQAaIJUBHEWknIqHAGGCG7wYi0hr4FLjeGLPJZ3mkiEQXPwcuBtYELKcuJ0B4CwP2FkopdaoJWBWTMcYtIncBswEX8JoxZq2ITHDWTwb+DMQBL4gIgNsYkww0A6Y5y4KB94wxXwUqr8UBwniKAvYWSil1qglkGwTGmFnArHLLJvs8vxW41U+6bUDv8ssDxhUCQJCWIJRSqoSOpIbSKiYtQSilVAkNEOATILQEoZRSxTRAAATZmjbxai8mpZQqpgECtAShlFJ+VClAON1Og5znnURkhIiEBDZrdcgJEEFebYNQSqliVS1BzAfCRCQBmAPcBLwRqEzVOacXk2gvJqWUKlHVACHGmKPA1cCzxpirgG6By1Ydc0oQoiOplVKqRJUDhIicDVwHzHSWBXQMRZ3SKiallKqgqgHiPuBRYJozGro98H3AclXXtIpJKaUqqFIpwBgzD5gH4DRWHzDG3BPIjNWpkgChVUxKKVWsqr2Y3hORGGfivHXARhF5KLBZq0NaxaSUUhVUtYqpmzEmG7gSO7dSa+D6QGWqzpXMxaQBQimlilU1QIQ44x6uBD4zxhRRyc1/TknFJQijAUIppYpVNUC8BKQCkcB8EWkDZAcqU3VOq5iUUqqCqjZSTwIm+Sza4dwm9PSgVUxKKVVBVRupY0XkPyKS4vz9G1uaOD04JQiXVjEppVSJqlYxvQbkAL9y/rKB14+XSESGichGEdkiIo/4WX+diKxy/haISO+qpq1VJW0Q2s1VKaWKVXU0dAdjzDU+r/8iIiuOlUBEXMDzwEVAGrBERGYYY9b5bLYdON8Yc0hEhgNTgAFVTFt7nOm+tYpJKaVKVbUEkSci5xa/EJGBQN5x0vQHthhjthljCoGpwEjfDYwxC4wxh5yXi4DEqqatVSK4CcalJQillCpR1RLEBOAtEYl1Xh8CbjxOmgRgl8/rNGDAMba/BfiyumlFZDwwHqB169bHyVLl3BKMS0sQSilVokolCGPMSmNMb6AX0MsY0we44DjJxN+u/G5oe0TdAjxc3bTGmCnGmGRjTHKTJk2Ok6XKuSUEF1qCUEqpYtW6o5wxJtsZUQ3wwHE2TwNa+bxOBNLLbyQivYBXgJHGmMzqpK1NHrQEoZRSvk7klqP+rvJ9LQE6ikg7EQkFxgAzyuxApDXwKXC9MWZTddLWNrcEazdXpZTycSL3dDjmVBvGGLeI3AXMBlzAa85U4ROc9ZOBPwNxwAsiAuB2qov8pj2BvB6XR0K0kVoppXwcM0CISA7+A4EA4cfbuTFmFnZyP99lk32e3wrcWtW0geQR7cWklFK+jhkgjDHRdZWR+uaRYIK1ikkppUqcSBvEacUTpFVMSinlSwOEwyshBGuAUEqpEhogHB4J0SompZTyoQHC4QkKJlgHyimlVAkNEA6vhODCU9/ZUEqpk4YGCIcnKIQQrWJSSqkSGiAcRrSKSSmlfGmAcHiDQgjRAKGUUiU0QDi8QdrNVSmlfGmAcGgJQimlytIA4dAAoZRSZWmAcJigEIK1m6tSSpXQAOHQEoRSSpWlAcJhgkIIEQ9ej5YilFIKAhwgRGSYiGwUkS0i8oif9V1EZKGIFIjIg+XWpYrIahFZISIpgcwn2AAB4HEXBvqtlFLqlHAid5Q7JhFxAc8DF2HvMb1ERGYYY9b5bHYQuAe4spLdDDHGHAhUHn2VBIiiAkIaHPdeSEopddoLZAmiP7DFGLPNGFMITAVG+m5gjNlvjFkC1PscF25XBAAmP6uec6KUUieHQAaIBGCXz+s0Z1lVGeBrEVkqIuMr20hExotIioikZGRk1DCrkBPZ2r7pgS013odSSp1OAhkgxM8yf/e3rsxAY0xfYDhwp4gM8reRMWaKMSbZGJPcpEmTmuQTgKyo9gDIgY3VS7j0Tfj2LzV+X6WUOlkFMkCkAa18XicC6VVNbIxJdx73A9OwVVYB44loSpaJgIxqBoiNs2DNx4HJlFJK1aNABoglQEcRaSciocAYYEZVEopIpIhEFz8HLgbWBCynQGSDELaYBKS6AaIgBwpyA5MppZSqRwHrxWSMcYvIXcBswAW8ZoxZKyITnPWTRaQ5kALEAF4RuQ/oBsQD00SkOI/vGWO+ClReAaLDgtnsTaDXwWrGoYJsGySMAfFXq6aUUqemgAUIAGPMLGBWuWWTfZ7vxVY9lZcN9A5k3sqLDgthsUkgJH8uHD0IEY2rlrAgB7xF4C6AkLCA5lEppeqSjqR2RIcFs8W0tC+qU82Un20fC7WaSSl1etEA4bABwinMZGyoWiJjbAkCbFWTUkqdRjRAOKLDQtht4igKCistQbwz6thdWN0FtnoJSgOFUkqdJgLaBnEqiWoQjCGIA5GdaLFnBRQega3fQdHRyhP5BgXtyaSUOs1oCcIRGhxEg+Ag0iK6QvoK2L0MjAey0ipP5FutpCUIpdRpRgOEj+iwELaGdgZ3Hqx4zy7M2QNer/8EZUoQGiCUUqcXDRA+osOC2eDqZF+s+cQ+egrhaCUTyvqWIAo1QCilTi8aIHxEhwWT6m0K4Y3AUwAhkXZFZdVMWoJQSp3GNED4iA4LJqfAAwln2gUdL7KP2bv9J9AAoZQ6jWmA8BHVIJic/CJISLYLul5hH7MrmWNQezEppU5j2s3VR3RYCLn5bug5CjI3Q+dLwdXgGFVMThtERLyWIJRSpx0NED6iw4LJyXdDfEcY9ZpdGNOy8iqm/GxwhUJkvI6kVkqddrSKyUd0g2ByC914vT73NYpJOHYVU4No+6dzMSmlTjMaIHxEh4VgDBwpdJcujE2ALKcE4SmCjV+WjosoDhChUVrFpJQ67WiA8BEdZmvccvJ9AkRMAuSkg9cDsx+D98fA5tl2nW8JQgOEUuo0owHCR5TfANESvG748mFY/JJdlvqjfSzIgQYx9k8DhFLqNBPQACEiw0Rko4hsEZFH/KzvIiILRaRARB6sTtpAiA4LASC3oKh0YfOe9nHJy9DhAmh9tk+AyHYCRJR2c1VKnXYCFiBExAU8DwzH3kZ0rIh0K7fZQeAe4OkapK11xVVM2b4liNZnwcM74Pfb4TefQrvzYe8qyM9yAkRxFVO2vT+EL2Pgu7/DnlWBzrpSStW6QJYg+gNbjDHbjDGFwFRgpO8Gxpj9xpglQFF10wZCdAM/VUwA4Q3tLUhFoO1AMF7YuahsGwTGThHuK2sXzP9X6cR/Sil1CglkgEgAdvm8TnOW1WpaERkvIikikpKRkVGjjBYrqWIqHyB8JfazYx9SfywXILAzvx72yXb6cvt4oBq3MFVKqZNEIAOE+Flm/Cw7obTGmCnGmGRjTHKTJk2qnDl/SnsxlS/Q+AgJt1NxrP/czvTaIBpCnQDx8c0w+VzIO2Rf715mHw9sPqF8KaVUfQhkgEgDWvm8TgQqGXFWq2lrLCLURZD4qWIqr/9tcGi7fd4gprQEsXcV5B+GnybZ18UliKxd2oitlDrlBDJALAE6ikg7EQkFxgAz6iBtjYlI6YR9x9LtSmjWwz73rWICOxPsz5MhZ5+9M11kU7s8c0sgsqyUUgETsABhjHEDdwGzgfXAh8aYtSIyQUQmAIhIcxFJAx4A/igiaSISU1naQOXVV9OYMPZm5x97o6AguOCP9nl0M9vNFaBFElz9sq16+uhGKMiyE/+BVjMppU45AZ2szxgzC5hVbtlkn+d7sdVHVUpbF9rFR7It48jxN+w8HH67COI7w5H9IEGQfBPEdYDBj9jurQA9rrElCm2oVkqdYnQkdTntm0SyI/MoHm8V2tObdrWliejmcM8K6HujXX7uA3ZAXUgktOgNjdrCgU2BzLZSStU6ne67nPbxkRR6vOw+lEfruIiqJ2zUpvR5kAt+/YHt8uoKsaUMrWJSSp1itARRTvsmtj1h64ET7HUUFgvNnYbs+I62kfrowRPMnePwTtj5c+3sSymlKqEBopz28ZEAVWuHqKpeo+2Ef3P+Wjv7m/M3mDq2dvallFKV0ABRTuPIUGLCgtl+oiUIX817wIAJsPSN2rny37sajmZC3uET35dSSlVCA0Q5IkL7JlElJYhDRwr599cbKXB7/G6fdugo69KrcLvRwY9CbCK8fRWs+6zmGXQXlDZ4H95R8/0opdRxaIDwo71PV9c3FqTy7HdbWLg10++2E2es47a3Uo6/07AYuOVraNYNPrwRdiyoWeYyNoJxgtWh1Oql3b++9O549e37f8JHN9V3LpRSx6ABwo/2TSLZm51PTn4Rny5PA2BVWpbfbdemZ7H7cB6ZuQXH33FMS7h+uu3xNP23pbO/7lgI+zdULXP715U+P1TNEsR7o+HT8dVLEyjrPoMtc+o7F0qpY9AA4Uf/dnEA3P72UnYdzAP8B4jDRwvZk2VHXa+pSjUT2FHXI1+wczm9Nxq+/we8PhymnA+rPz5++n1rwNUAGsRWrwRx9KCtktrxY9kZZ+tDQa4dOFiQZe+roZQ6KWmA8KN/u8bccHYbFmzNJDLUxSXdm7Eq7XCF7dbvKb3N6Jrd1TjRtR0II561NxKa9yR0uczO4fTJLbBtrr3R0N414C6smHbfWmjaBRq3KxsgcjPAc4xJBvesLH2++qOq5zUQ9q6299SA+g9WSqlKaYCoxB8u7crZ7eO48Zy2nNU+jv05BewrN0fThr221NAwIqR6AQKg7w1wzzL41dv27zefQsM28NWj8P0TMHkgPH0GfPlI2fET+9baiQIbtS1tpM7Pgkl9YOFzZd/D6y19XhwgmnSFVR9WvPtdXSqe5RbsmI5A2reu9BaxSqlq0QBRibAQF++PP4uHLulMr8RYAFbuOlxmmw17coiPCmVgh3jWpNegqiQyHrqNsNN1hITBxX+zbQzz/wVdR0DHS2DxS/DsmbB9vi1V5O6Dpt1sO8bhneD12Lr8whzY/E3pvpe+Cf9qX3pvij0rIbY19L8VMtbDC2fDmk+qnteNX8Fnd5UNLItfrtntVPesgFBngsOsAJcgvvs7fHp7YN9DqdOUBojjEBG6tYjFFSSsLldKWL83my7NY+iREMuug3kcPuqnSqg6uo6ALpfbv1GvwTUvw+0/QFQzePdaeGuEfd79SluC8BTau9ht+sqmT1sMhUftne7m/NUGh+KG4L2roEUv6DsOhv2fvX3qZ3dD7n570j9W9RTAsrdg+duQ4TSmHz0Isx6Euf9X/c+ZvhzaDYLgsMCXIA7vhOy0ireDVUodlwaIKggPddGpWTRLdxwqWebxGjbuzaFL82h6JtgSxprdVWyorowIjHnX/rns7U9p3gPGfWGn6wgKgXEz7XiKRm3t+sytsPlriG1lA8aun2Hh83D0AIRE2OCRn22n+miRBK5gOOsOW63lzocvfw+Tz4N3r6k8X8ZA2hL7fMMX9rH49dbvoCiv8rRLXoXl75a+zs+281K17GPzHOgSRPH+M7ee2H683pp3TVbqFKUBoooGdYxnSepBcvKLyM4v4qmvNlDg9tK1RQw9E2MRgZQdtTTXUnmR8XDb97bNIr6jXdbQmRxw3lO2pDD4EQgKhiWvwE//s6WRblfaaqfiOv8WvUr3GX+GbQdZO832jNo2t/QWqeVl7bJTmgNscGZg37nIPrrzYNs8/+kKj8DXf4LZj5YGkb2rAGMDRMNWFUsQ+Vl2vEZtKMixd/iDE79h0+qPbG8z3/YTdWpKS4Flb9d3Lk4JAQ0QIjJMRDaKyBYRecTPehGRSc76VSLS12ddqoisFpEVIlKFkWiBdWHXZhR5DPM3HeDWN1N4af42RvRuyWW9WhAbHkL3ljGVDqarFa4QCI0sfd2oHSRdBzt+st1eu46wPaE2fGHvcDf8Keg8zJ4gP7nF1vknJJfd55DHIPkWuHWOXb/kFf/vXVxa6HoFpC+D7HRbUmne096Pe2Mlt+3Y+CUUHbEn/fVOyaMkWCXZEkT5Xkzf/xOmDLEljROVlVb6/EQDRPFn3LvmxPZTmwpyIWdvzdJm7bYDNvMOQ1G+vU1uQc5xk50WfnoGZv7Ott/Vtp2LYO302t9vPQlYgBARF/A8MBzoBowVkW7lNhsOdHT+xgMvlls/xBiTZIwpd2are31bN6RhRAj//mYji7cfZOIV3Zg0tg9hIS4Azm4fx/Kdh8kvCsCPzp+gILjyBbh7Kdz8lR2p3eFCW4q49g2IaQHth9hqqYJcO/14ZFzZfUQ1gcv/A4lnQu8xdhxGtnPrb2Pgh3/DJ7faH31wGJz/sF237G3YvRTaDoKOQ2H95zD/aVjxXtlG61UfQkyCrQ5b9qZdlr7cBoaoJtCwta0KKzxammbHT7ZUsvnrqh2HtKXwr47+q5BqK0C4C21VGpS2wZwM5vwFXr6wZj3SNnwB66bbEub6z+GbP9XeZJJVcfQgvH4ZHKiHW/Hu3wCegurPRFAVX/8JvrivfnsJ1qJAliD6A1uMMduMMYXAVGBkuW1GAm8ZaxHQUERaBDBPNRbsCmJI56ZsyzhCi9gwxg5oXWb92R3iKPR4WebTTlEn4jpAglPwGnivDRhtzrGvw2Jg1Ku2DaPtucfez4AJtg1k8rm2imrWg/aEsfoj247QIsl2r+00DOb+w7ZftB4Afa4HDHz3N5h+B7w0yAaUI5mwdY69o16f6yH1B3sST18OLZPsezZ0jmHxibwg11Z3Qel8VbkZNk9L3/Cf79Uf2eqv4gDkq7j94Xj34zi049hVDjsXQkG2vWtgxkl0Z8Ddy2wDvG8grMyyt2DlB6UnrvQV9jH1B9juVBEuftkG3LqwfZ4dtFncplVXivLhoHMxUVtVmcXys+2FU96hqn0np4BABogEwLf+IM1ZVtVtDPC1iCwVkZNifoiLujUD4LeDO9Ag2FVmXb+2jXEFCQu3lVYzeb2GNxek8v3G/Xircoe6ExUSVtp4XazbSEisQgEsviOMn2vbNr75s61uOvMm6PMbO/dTYrINIFe9BHFn2DStBsAZF8LDqfCHPXBXim0vmftPG0S8blsySbrOnlwXPgcHt9n2B7AlCYClr9vxHenL7AC6xh3slW3eYfjwBjuwbnEl1V/FJY2VH1TsiXV4ly1RtR1og1NlV3VfPQoz7qq8fWHz1+AKtcHxZAkQxpSWZvasKLtu+Tvwn272ZAh2gseZv4Np421PuKL80jQ7frIn63bnQ3QL+PqPdZP/4vau9EravQIlc3PpIM2MWg4QO34qnSdtr5/u30X59ntZ8mrtvm8ABTJAiJ9l5f9Dj7XNQGNMX2w11J0iMsjvm4iMF5EUEUnJyMioeW6rYFj35rxyQzK/HtCmwrrosBB6JMTyzqIdXPLf+Uyas5m/fL6Wx2es5abXl3D5sz+Sk18U0PydsKZd4bbv4KGtcN8auOIZ25bR9wbo7dx/IryhHdR39Sv2VqvFQiNskBl4n23wXvIKnHUnNOtuq7s6XlJaCigOEE062xsrLXrBVpWsm2GXD33cVjM90xN2LoC258G+1RXnnsrcaq8G250PuXtLq4GKZaXZ+a/iO9tpPY74+X0c2FzavrD4FTte5Os/2ZMq2HrqdTNsHlr2haydtqRTF44ehJkP+r/RVNYuKHTyUVwaKLZhJmTvLg14e1bZHm6dL7PjadZNt8ElvLGteju803atPnOcLS3l7q9a/o4cKDv2xp+iPHv8ygfv4rztruNG/+JSQ1BI1ec/O56MTfY72DbPVsVKkP/xQbsW2e+lePzR3Cft+KKTWCADRBrQyud1IpBe1W2MMcWP+4Fp2CqrCowxU4wxycaY5CZNmtRS1v0LChKGdmuGK8hfXIPfDGhNQqNwYsND+M83m3hz4Q5uGtiWp0b1YsPebP76+Tq/6U4qIrYU0ND5WkIj7bQgxXfHAztIr9e1/tMn3wxRze1gvgv/XLq87w2lV24tkuxjRGMbjCb8CN4iWPIyxHeyJ7IOF0D7wTDmfbjif3b7jV+Wfa/ik9OlT0NEnL2J0tOdSntaZaXZwYHxTonHXzvEwudt6aDL5bDmY3hnFCyYZAcrgq2fz9oJyTfZgAZl7y9ujM3H7Mdse83M31XepTZ9uT0peL32pPLVo7ZEVZkf/m2PycqpFdf5nuh8SxDG2A4EYE/2YMfHAFz6FETE27Erxgv9bi1N124QdLkUMBWPc2VmPQjvjjp29d33T8CH19vPUszrscciOMwe29xqXNh5PXBwe9llWbvhqz/Ac/1h03Harvavd0qV59ZOe1LhEXhrJLx6kf39tD7blrB9SxAHt5cOaAVbBZv6ky1lT/21Dei+PO7jN6AfOVDxOARAIAPEEqCjiLQTkVBgDDCj3DYzgBuc3kxnAVnGmD0iEiki0QAiEglcDJxE3Uf8uza5FV/cfR4fTjib12/qxx8v68qfLuvGr5JbccfgDny0NI2Zq/bUdzYDKzQCbp9npzYPCStd3vFiGzgatrGBoZgrxPaGOtOZ+rtVfztW4/ppMPpte9KK62BLAes/tyfU/Gx7lbvyfYjrCE06wVVT7PiOqGbwwW9sg3vWLjtmpGl3e1W34Dl7YpoyxLZxHNxmG9Z7j7H363Dn25LDGRfBD/+xU3QseNb2GOt8KTTpYvOYlmIDy+Fdtp3m3VGweIrt7bX8XXhtmK0yA1si2TDTBqt3f2VPCivegc9+a0tOz/UvW+Wwfz28Ntx2Py5evm56xeNcHCA6XWKvXourzzK32JtJgU+AWGKr82ITbU+0Q86Jpc9v7KSPUc1s8GvWwwbU4hJV4VFIeb10NH6Z999Q2ltneSXtN5lbYdFk29Nt/lOlJZ0Dm23pp+co+7o61UwLJtmZBTKcIH1wm233WvwSFB2F935lv+fK7F9vfzPNe9pAf7wBosfz0/8gJ92Wpo9kQPvzoXmv0hJE5lZ4Ltl2KNj6vb2QMR6Ycbe9MGne0057XzwN/5FMeGEAfHanfZ2xyf8Fx8c329/Zieb/OIIDtWNjjFtE7gJmAy7gNWPMWhGZ4KyfDMwCLgW2AEeB4hsENAOmiUhxHt8zxpzcZbFyhnRuypDOTUte33thJ37aksn9H66gUUQI55wRX4+5CzDfqqdirmC48nn/ExACnP97W9ff+VL/67tcCj/+1845hUBwA3uVVVy66DjU/hXk2Flyp02w/4ixibaKa9j/2UGBG2cBBqbfaSc9DG5gx5DEtLQlkRZJtsTx4kB44zK770ufhiCXnSAxKMSO6/C6bVuN122rZoY9aQNixkZ4cwS8eI7twZXt/OMHBdsTQtPu8MUDtsR00V9tddyXD0NiPztOZd5Ttlpt5wIb1HqNgVVT7Qkk1qcJb/96iG5pS1kbvrDvE5tYGhTaDLR3L/R6YNcSG3jBjsJf+rotSTRsDefcbfMtUnqcl75hA+y8J+1JdM9KW93oa/6/7EDMhL42yA75IwSH2uO/f71Nk/K6LSXcNsdeZU+9Dm6YXhoQzrzZpt29zAa6vWtsG0H3q/z/BrxeSHnNfq8Ln4Whf7FB13hgwk/280yfAF8/ZrePamp/U7uXQefhcPZdtt2hZR9bneoptAGmURv7O6iuzK22e3D3q2HYP+1FRdJ19sJlzce2anDRi/Y3svAF+50P+aPN+8GtdpzS0IkwKckG2fN+Bx/daIP8we0w6CF48wr7/dy11P4PgQ0axR0Lts+FM4ZWP+9VFLAAAWCMmYUNAr7LJvs8N8CdftJtA3oHMm91LTQ4iNfH9WPMlEXc+Ppizj0jnqv6JjK8R3NCXP4Lchv2ZpNX6KFP60Yly7Zm5NIyNpzwUJffNCe1Y/2Qo5rCfceY1+nsu20jamiUvRrP3WdLDMUDB4s1iIbR78CUwXYyw9hEu3zA7XY8xs6FMPgP8M7V9sr6sv/Y4ADQ/7bS/Uz40XbT3bfG/tODLe3Ed7QnzSsm2fShkXDJP2wAAXslftt3sOoDeyI867f2BJTymi0lNWxlSzCJ/exnSvoNvHg2fHqb7QCw7jMbcPKzbbqk62yAWD/Dfl53gdNAvd4GuOLquk2zod8tNiiEN7I9x6ZPsO0y2WmQ6PybtTkXIpvY9hQROP+hssevy+Xw82Q7dia6pf3Olr9te8g1bme32bHQ1qMPvNf2mHvvV/Djf2xJ5NvHS6dwb9wBRj5rj8mvP7TH/NWLbRVmaJTtzRbf2Wn3yLB3Wzyy375v6wGQvQc2z4ZmPW1X7G3f2/aSxu1ttVv6cvsd3/CZPRYA17xqj1FxkIiIt+1gi16wf8Zrj2lxafCVofZ7veVrW7p159nf0OGdNnh3uLBsYC6Wd8heiISE2UAf3dxW4YEtQYD9/ax415aeU51q1E4X29/Uuun2O2rczr7H0jftbzr1BxtEvn/CHq9cZ5zLmk/sZz2aaT9fUAiEhMOqj2y+dy+1JeFaJuY06a8LkJycbFJS6n1M3TEdyC3gpXlbmbV6L7sP59E4MpQuzaMZeEY8o/u1Ij7KXsnkFrgZ8vRcDh4p5D+/6s3IpAR+3pbJ6CmLCA9xcXmvFjx2WVcaRoTW8yc6Se1ZZW+ONOpVe4Iob8scewIY+hc7pqSqiqfbKO5KXBPb59sTY7TtFcfW7+zJprje+d6VpW1AYEszxb2/0lfY0kjRURv0LviTrWI5sNE2pGdssAMihz8J/+tle7UdSoVbvoVW/ez+9q62J8HyPd7ABp8tc2w1YLPu9ip4UpI92YdG2LE1az6xJ7s7FtoSwisXlraDtD4HBt5jg0KjdqUlE7BjHmY/aj/DGRfCVZPh24m2ZBgRZxv/wxvZ927aDdZ+Wtpu1eosWy2Vs8dON/PC2fY4jHnXlj58FeXZAN28J7ToY7/fg9vtyXrbXLjs37at66Xzbemy+Hi4GtgSTI9RtuSRf9iW4s64yJa83AX2+0/90QYIr9sGp7YDy75/QY7tml08zmLCT/Y7X/Ox/R52LbLdqkc+Zy8sNsy0bRFgS3QX/92WjDbPtsc7d7+tFizyGS/U4xobZFd/ZEs/rga2i3uDqMp/d5UQkaWVjTXTAFFPvF7DvE0ZzFy9h037cliVlkWD4CBeuv5MBnduyr+/3siz322hW4sY1u/N5h9X9eTdn3dwMLeQwV2a8uGSXcRFhfLqjf3o4cwFVZvyCj2nZinlVJW+3NYrtz0PRkwqu27/BntVn77MjpY/vAu2fGMndOxxjT0hLnrRXrFmbrYnwL43wvtjbXVFXAc791ZwDS8m5j4JKa/a0thuZ5zEDTNsfTvYqp99a+zVf/sLqhdwvV57Zf/d3+GSJ+yV+NRf28DTfzz0vNZWpxSX5gY9BEP+YLvyxrYqzcOJ2LXYVgvGJthG5pXv22nxh/+fDd7L3y29kg9vbINbRLydqaD9YP/7LMqzbUjufBj04LHf3+O2VZKN29uAF+SyPaLeGw03f2kDzUfjbAeQjhfbYzXyedtA/salNnj/5pPSEl41aYA4BWzZn8s97y9n24FcxvRrzfuLd3JJ9+Y8eU0vJryzlHmbbE+P/41JYmRSAmt2ZzHeuRf21PFn892GfQzu3JS28XY6jtVpWUxbvpuHh3cuM2Yjt8DN9OW7uaJ3S2LDQ/zmZePeHK58/iceuKgTtw1qH+BPrkoU/y/6XnVX5uB2WwIov63XU1rdFQh7VtnR9p2H1e5+Pe7SOvZNs23ppbh6sJi70FYHVeX4VNeRTNvl2hVsr9jDGpYGVI/bltAaxNjqyEAcX3eBbaPy/Wy+32XGRtu47ht8jbGTcbYaULbjRzVpgDhFZOYWcN0rP7M1I5c+rRvxvzFJtIgNp8Dt4dFPVpOd72bK9WcS5HSzXZ2WxTWTF1Dk8WIMtGoczvTfDsTjNVz27I9k5BTw0CWd6ZEQy8vzt/F/1/Tk+e+38P7iXTSPCeO/o5M4u0NchXzc8/5yZqxMJzhI+PiOc0hq1bDWP+uRAjd7svJIbBRRMl2JUqruaYA4hRhjKPIYQoOrVkz/YlU6M1ft4YIuTfnj9DU0jw3D4zVk5hbSIyGmZAryvCIPCQ3D2X04jyt6t2RtehZ7s/L59LfnEBseQvrhPJpGh3G00MPw/81ndL9WzN90gKAg+OzOc9mReYTlOw9zw9ltCK6kUd2fQreXIAFXkPDC3K3szDxK/3aNeWr2BvZlFxDqstVqQ7o0rZD267V7SdlxiEeGdSkJikqp2qUB4hfim3X7eGneVhqEBHHree3p2DSKof+ZR9PoMB68pDMPfLCCZjFhfPPAIHLy3Vzx7I8UuL3k5BfhOxNIaHAQPz48hN2H8hg9ZRHt4yPZduAIhW4v53SIY9LYPsRHNWDpjoNMnreNdenZPPvrPvT16W0FkJVXxOiXFpKT7+bsDnF8vDSN4CDB7TW0bxLJ7YPa89K8bbiChK/uG1QyAPHQkUJemr+NyfNs/+/3bzvLb0nnRKQeOMLKtMOMTCrtobJy12HaxkdWWvWm1OlIA8Qv2Jb9OTSKCCUuqgHLdh6icURoSTvF8p2HePiTVQzp0pSz2sWRkVvA/ux8OjaL5pLudizDjJXp3PP+cvq0bshVfRL4+8z1xIQFc3aHeD5fmU6jiBAiQoPJPFLAr/u3oW18BKP72R44N72+hCWpB2ndOIKtGUf4VXIij13WjWU7DzGgXWMiQoP5YlU6d723nElj+zCid0te+WEbT83eSKHby6gzE5m9di9Duzbjv6OTjvk5P1uxm6y8In7dv3WFEk5eoYenv95Iv7aNudiZT+uK535kbXo27946gIFnxDN77V4mvLOUS7o1Z/L1Z9byt1B1xhhWpmVxtMBN5+bRxEXVoH++H3mFHl7+YRtfrtlLl+bR/OasNpzZphFLUg9yIKeAQZ2akFvgJrJBMFENar/3+4yV6UQ3CPZbUgw0YwxvLEglN9/N3Rd2PH6CXxgNEOqEbNybQ5s421awYW8293+wko17s7ltUHvuu7ATuQVu7nl/Oct2HqLA7aVP64bkFXrYsDeHf1/bmxFJLVm+8zBntmlUYZoSr9cw/H8/cCC3gPM6xjN9RTpDuzbjdxd3omuLGB6btpqPl6bxzq0D2Jl5lMt6tajQZpGRU8C5T35HgdtL95YxPHhxZwZ3boIz0JK/fbGOV3+0o4d7JcYyrEdznvpqI+EhLhIahXPXkDN45NNVGAOFHi9zHxxMm7jIMu+xOi2LV37cxkOXdCaxUUTAjvVz323m6a/tKOGeCbHMuGtgyeeoKa/XMOL5H1mzO5u+rRuy3SkNPnppV/76+ToKPd6Sbds3iWTm3ecRHuoiI6eAO99bxn0XdjyhgZ1vL9rBn6avIbpBMD88PKRM1+zi88+JfsZiHqcoXPw7O1Lg5vcfr2LmajuDwas3JnNh12a18l5gq1DveX85bq+XKdcnExQkuD1ePkxJ48KuTWkWE3b8nfix6+BR3vl5B3cNOYPosMCWaDVAqFpV5PGSmVtI89iKP/6Zq/bw0McriQgN5slrelbpn3FdejZ/n7mOBVszubxXC54ZnVRSCli56zAjn/+pZNtWjcP57eAzuKhbs5IxI//8cj0vz9/GHy/rxqs/bmf34TzObNOI//4qiS0ZOdzyZgpj+7cmuU0jJs5YS3a+m64tYrh/aEfGv227bZ7RNIr/jUniyud/4qJuzQhxBdErsSE3D2zLgq2Z3P72UnIL3LSLj+SdWwfQMjbM70nt23X7eO77LRwtdDN1/Nk0jvTftdQYw+LtB3ntp+3kFrhpExdJ1xYxTJyxlou7NaNL8xj+++0m3rq5P4M6VT7HmMdr+HRZGud2jKdFbLjfbb7fuJ+bXl/CP67qya8HtGZvVj4jn/+RfdkFtG8SyZ8u78aKnYcB+N+czdw0sC2PX9G9JLC2bhzB1/cPqlFngk+XpfHAhyvp17YRKTsOccf5Hfj9sC4leb/jnaUsTj3IpT1b8JsBbejWMgaA9MN55Ba46dg0qsrBw+s1jJmyiO2ZRxjbrxUdmkbx4tytbNqXw0OXdGH68t1k5xfx1X2DaqUa0RjD7z5cyafL7Wj54uP78vxtPDFrPa0bR/D++LNIaOj/eynP7fGy4+BRYsNDGP3SQrZmHOGeC87ggYs7+93e4zV8lLKLoT7/CzWhAULVqf3Z+YSFuoip5pXP3qx8mkY3KNMgbYzhr1+sIyYshN6tYnnyy41s3JdDkEBym8b0Sozl/cU7uaBrM54d24cij5dPl6Xx95nryS1wYwy0bhzBl/eeR2SDYHZmHuXJrzZwy3nt6NOqIR+m7KJZTBiDOjYhKEh48KOVfLw0jRCXUOQxdGsRw7o92ZzRNIoHLurE7z5cSV6Rh4hQFy1iw2jZMJzERhGMTGrJ9xv389K8bbRqHM6+7AKSEhvy9q39S7oZr0o7zIa9OYzo3ZKnvtrIaz9tJy4ylNZxEWzZl0tOgZuEhuHMuvc8wkKCOP+pubSJi+CpUb2ICQuhkU+w2ZuVT1AQ/HPWBqYt303L2DDeuXUA7eIjEREOHinkh80Z9EiI5a+fr2Pdnmx+eviCks4Pa9OzeHbOFv5waVdax5WWiCbOWMsbC1K5b2hHXpy7lY7NolizO5vrBrRm4BnxLNtxiCKPl8cu60aIS0jPyiczt4CFWzPZtC+Xjs2iuKBLUzo2jWL6it387sOVnNU+jtfG9eOhj1fx7bp9fH3/IFo1juCJmet4+YftnNMhjmU7D5Ff5KVNXAS5+W4yj9gpWRIahnP92W0Yd07bSgOUMQZjYPqK3Tzw4Uq6t4xhbbrtnBEbHsKzY/swqFMTlu88xKjJC2kRG8bVfRNZseswA9o1Ztw5bYlsEMyhI4Ws3p1FUuuGVfrtFge/By7qxMKtmaxJz+KR4V342xfr6N4ylk377B36LunenDsGd6BDkyg8XsPq3Vlk5xVxXsd4RITtB47wwZJdfLIsjYwcO4twiEvo2iKGbRlHmP/7IX4vNIrff0C7xrx321mVTiJ6PBog1GnDGMO6PdnMXruPb9btI/XAEQA+u2sgnZpFl2y36+BRXv8plS7No7mkR/MqXzFm5BTwxap0rkxK4PUFqbz+03bGndOW28/vQFSDYDbszeaHTQdIz8pjz+F89mTlsS3jCDkFdtK035zVmolXdGfm6j3cO3UFA9o15r6hnXh/8U5mrLSTGTeMCOHw0SJuPLsNj17albAQF4VuL4u2ZdImLqKkeuuVH7bx95l2Ur4GwUGM6deK9k2i+GFzBt+uL52Se9w5bZmxMp2DRwoRgVBXEEUeL15jnxd6vNw3tCP3De103M+fV+jh7veX8+36fbiChO9+dz7//WYT01fYvBfvb1j35uQUFPHTltL7n8RHhXIg157YW8aGkZ6VT9/WDXn7lgFENghm+4EjjHjuRxoEu+jWMob5mzK48ew2/GVkD7KOFvHe4p2s2Z1FTHgwnZpFEx7i4vNV6fy0JZMWsWE8cVUPLujSDGMMRws9HDpayIKtmTz11QaCg+xnTmhku3pn5RWxNzuf5jFhZQJrSupBHvxoJamZR2kTF8GOzKM0jgxlbP9WfLw0jX3ZBbiChPM6xjOmXysu7tbcbw+6/CIPF/57Ho0jQ/nszoHsOHiUX7+8iD1Z+UQ1COabBwaRlVfElPnb+HrtPtxeLxd3a84PmzM4dNRO+9+/bWMQWLz9IK4gYUjnJlzQpRk7Dx5l4BlxNIsJ45Jn5nNVnwT+cVXPMgGyyONl6H/mkZVXxOGjRTx4cSfuuqBm7SsaINRpzes1AesGa4w5bhVHfpGHT5ftxmsM1w1oXbL9tOVpPPLJagrcXiJCXdw0sC19WjXiX7M3MrhLEx4Z1uWY+84v8vDOoh3EhIewZPtBPl2+G4/XEBsewrhz2tIoIoSERhFc1K0ZqQeOMHP1HvKLPBS6vYSHuji7fRxvLkxl4dZMZt8/iKbRVasPN8bw+ao9uD1eru6biNdr2JqRS4HbS4cmUby1MJV/frmByFAXd13QkXbxkfRIiCGxUQQZOQV8sGQnP28/yIjeLRmZlFCmy/amfTnc9PoSsvKKuG9oR24a2O64V76LtmXy58/WsGlfLhGhLtweU6bdpG/rhoSFuFiSepCp48/izDbHHjRW6PaSW+CmcWQoy3Ye4r/fbOKHzQdoHx/Jg5d0ZlVaFp+t2M2erHy6tojhmr4JdGgaxYB2jdl9KI9v1+9n074cpi3fzTu3DODcjrZ9xuuUDsJCXHRuXnqxsj87n4c/WcWS1ENc0KUpQ7s1Iye/iH/N3kjD8BB+1a8Vo/om0tRPe8U/Z63npfnbaN04gr+M6E6vxFi+Xb+PlNRDfLQ0jVduSGbGynQWbcvk+wcHE1mDDgYaIJSqJ+vSs0nZcZCRSQknXO+dV+jhaKHtaVSd9oBABNBv1u2jS/NoWjWufoN9boG7JNBVVYHbw7uLdrInK49gVxANw0NoGBFCy4bhDOwQT1CQkF/kqfGgy037cmjVKKJkehmP1/DFqnT++80mUjPtHEjF1Y7FLujSlNfG9avye5S/2KhqA/2CLQf402dr2JpxBFeQlDTEX9ytGS9dfyY5BW7yCz1+A0xVaIBQSqkaMMZw+GgRa9Ozmb85g8aRoVzTN5HIBi7CQ1y11vvqeArdXt5etIPM3AKu6N2SM5pGVToLdHVpgFBKKeXXsQJEIO8op5RS6hQW0AAhIsNEZKOIbBGRR/ysFxGZ5KxfJSJ9q5pWKaVUYAUsQIiIC3geGA50A8aKSLdymw0HOjp/44EXq5FWKaVUAAWyBNEf2GKM2WaMKQSmAiPLbTMSeMtYi4CGItKiimmVUkoFUCADRAKwy+d1mrOsKttUJS0AIjJeRFJEJCUjI+OEM62UUsoKZIDw1/+rfJepyrapSlq70JgpxphkY0xykyaVz1mjlFKqemp/Xt9SaYDPnddJBNKruE1oFdIqpZQKoECWIJYAHUWknYiEAmOAGeW2mQHc4PRmOgvIMsbsqWJapZRSARSwEoQxxi0idwGzARfwmjFmrYhMcNZPBmYBlwJbgKPATcdKe7z3XLp06QER2VHDLMcDB2qYNpA0X9V3suZN81U9mq/qq0ne2lS24rQaSX0iRCSlstGE9UnzVX0na940X9Wj+aq+2s6bjqRWSinllwYIpZRSfmmAKDWlvjNQCc1X9Z2sedN8VY/mq/pqNW/aBqGUUsovLUEopZTySwOEUkopv37xAeJkmVZcRFqJyPcisl5E1orIvc7yiSKyW0RWOH+X1lP+UkVktZOHFGdZYxH5RkQ2O4+N6jhPnX2OywoRyRaR++rjmInIayKyX0TW+Cyr9PiIyKPOb26jiFxSD3n7l4hscKbZnyYiDZ3lbUUkz+fYTa7jfFX63dXVMaskXx/45ClVRFY4y+vyeFV2jgjc78wY84v9ww7C2wq0x07vsRLoVk95aQH0dZ5HA5uwU51PBB48CY5VKhBfbtlTwCPO80eAJ+v5u9yLHfRT58cMGAT0BdYc7/g43+tKoAHQzvkNuuo4bxcDwc7zJ33y1tZ3u3o4Zn6/u7o8Zv7yVW79v4E/18PxquwcEbDf2S+9BHHSTCtujNljjFnmPM8B1lPJDLYnkZHAm87zN4Er6y8rXAhsNcbUdCT9CTHGzAcOlltc2fEZCUw1xhQYY7ZjZxLoX5d5M8Z8bYxxOy8XYec7q1OVHLPK1NkxO1a+RESAXwHvB+K9j+UY54iA/c5+6QGiytOK1yURaQv0AX52Ft3lVAW8VtfVOD4M8LWILBWR8c6yZsbOnYXz2LSe8gZ2vi7ff9qT4ZhVdnxOtt/dzcCXPq/bichyEZknIufVQ378fXcnyzE7D9hnjNnss6zOj1e5c0TAfme/9ABR5WnF64qIRAGfAPcZY7Kxd9nrACQBe7DF2/ow0BjTF3uXvztFZFA95aMCsRM6jgA+chadLMesMifN705EHgPcwLvOoj1Aa2NMH+AB4D0RianDLFX23Z0sx2wsZS9E6vx4+TlHVLqpn2XVOma/9ABRlSnJ64yIhGC/+HeNMZ8CGGP2GWM8xhgv8DIBrIo4FmNMuvO4H5jm5GOf2DsA4jzur4+8YYPWMmPMPiePJ8Uxo/Ljc1L87kTkRuBy4DrjVFo71RGZzvOl2HrrTnWVp2N8d/V+zEQkGLga+KB4WV0fL3/nCAL4O/ulB4iTZlpxp27zVWC9MeY/Pstb+Gx2FbCmfNo6yFukiEQXP8c2cK7BHqsbnc1uBD6r67w5ylzVnQzHzFHZ8ZkBjBGRBiLSDntP9sV1mTERGQY8DIwwxhz1Wd5E7D3hEZH2Tt621WG+Kvvu6v2YAUOBDcaYtOIFdXm8KjtHEMjfWV20vp/Mf9jpxjdhI/9j9ZiPc7HFv1XACufvUuBtYLWzfAbQoh7y1h7bG2IlsLb4OAFxwBxgs/PYuB7yFgFkArE+y+r8mGED1B6gCHvldsuxjg/wmPOb2wgMr4e8bcHWTxf/1iY7217jfMcrgWXAFXWcr0q/u7o6Zv7y5Sx/A5hQbtu6PF6VnSMC9jvTqTaUUkr59UuvYlJKKVUJDRBKKaX80gChlFLKLw0QSiml/NIAoZRSyi8NEEpVg4h4pOwMsrU2A7AzM2h9jdlQqoLg+s6AUqeYPGNMUn1nQqm6oCUIpWqBc4+AJ0VksfN3hrO8jYjMcSafmyMirZ3lzcTeh2Gl83eOsyuXiLzszPf/tYiE19uHUr94GiCUqp7wclVMo33WZRtj+gPPAc84y54D3jLG9MJOiDfJWT4JmGeM6Y2998BaZ3lH4HljTHfgMHakrlL1QkdSK1UNIpJrjInyszwVuMAYs82ZUG2vMSZORA5gp4socpbvMcbEi0gGkGiMKfDZR1vgG2NMR+f1w0CIMebvdfDRlKpASxBK1R5TyfPKtvGnwOe5B20nVPVIA4RStWe0z+NC5/kC7CzBANcBPzrP5wB3AIiIq47vuaBUlejViVLVEy7ODesdXxljiru6NhCRn7EXXmOdZfcAr4nIQ0AGcJOz/F5giojcgi0p3IGdQVSpk4a2QShVC5w2iGRjzIH6zotStUWrmJRSSvmlJQillFJ+aQlCKaWUXxoglFJK+aUBQimllF8aIJRSSvmlAUIppZRf/w86dtjr+bKqaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이거는 hyperband tuner 사용하는 방법\n",
    "tuner = Hyperband(build_model,\n",
    "                     objective = 'val_mae',\n",
    "                     max_epochs = 200,\n",
    "                     factor = 3,                              #SNN 모델의 경우 factor의 갯수는 2\n",
    "                     directory = './Data',\n",
    "                     project_name = 'hyperband')      \n",
    "\n",
    "tuner.search(x_train, y_train, epochs = 50, validation_data = (x_val, y_val))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print('The hyperparameter search is complete. The optimal number of layer is', best_hps.get('num_layers'))\n",
    "print('The number of units in each hidden layer is')\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(best_hps.get('units_'+str(i)))\n",
    "print ('The optimal learning rate for the optimizer is', best_hps.get('learning_rate'))\n",
    "\n",
    "'''SNN 모델 node 개수와 학습률에 대해서만 최적화할 경우 사용\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print('The hyperparameter search is complete.')\n",
    "print(best_hps.get('units'))\n",
    "print ('The optimal learning rate for the optimizer is', best_hps.get('learning_rate'))\n",
    "'''\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history=model.fit(x_train, y_train, epochs = 200, validation_data = (x_val, y_val))\n",
    "\n",
    "# 교재 Fig. 3-9\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
